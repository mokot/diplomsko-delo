{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Å olar corpus data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.reading import read_xml\n",
    "from utils.logging import get_logger\n",
    "from utils.solar_id_enum import SolarId\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logger\n",
    "solar_data_logger = get_logger(\"Prepare Solar Corpus Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SOLAR_DIRECTORY = \"../../data/solar/\"\n",
    "SOLAR_FILE = \"../../data/solar/solar_complete.xml\"  # complete solar data\n",
    "MSD_INDEX_FILE = \"../../slovene_specification/MSD_index.npy\"  # MSD index\n",
    "\n",
    "# Solar Text Encoding Initiative (TEI) TAG\n",
    "SOLAR_TAG_TEI = \"{http://www.tei-c.org/ns/1.0}\"\n",
    "SOLAR_TAG_TEI_HEADER = SOLAR_TAG_TEI + \"teiHeader\"\n",
    "SOLAR_TAG_TEXT = SOLAR_TAG_TEI + \"text\"\n",
    "SOLAR_TAG_STAND_OFF = SOLAR_TAG_TEI + \"standOff\"\n",
    "SOLAR_TAG_GROUP = SOLAR_TAG_TEI + \"group\"\n",
    "SOLAR_TAG_TEXT = SOLAR_TAG_TEI + \"text\"\n",
    "SOLAR_TAG_DIV = SOLAR_TAG_TEI + \"div\"\n",
    "SOLAR_TAG_BIBL = SOLAR_TAG_TEI + \"bibl\"\n",
    "SOLAR_TAG_P = SOLAR_TAG_TEI + \"p\"\n",
    "SOLAR_TAG_DATE = SOLAR_TAG_TEI + \"date\"\n",
    "SOLAR_TAG_NOTE = SOLAR_TAG_TEI + \"note\"\n",
    "SOLAR_TAG_TERM = SOLAR_TAG_TEI + \"term\"\n",
    "SOLAR_TAG_S = SOLAR_TAG_TEI + \"s\"\n",
    "SOLAR_TAG_W = SOLAR_TAG_TEI + \"w\"\n",
    "SOLAR_TAG_PC = SOLAR_TAG_TEI + \"pc\"\n",
    "SOLAR_TAG_SEG = SOLAR_TAG_TEI + \"seg\"\n",
    "SOLAR_TAG_LINKGRP = SOLAR_TAG_TEI + \"linkGrp\"\n",
    "SOLAR_TAG_LINK = SOLAR_TAG_TEI + \"link\"\n",
    "\n",
    "# Solar attribute name for identifying the solar text\n",
    "# Solar id: solar{#div}{s-t}.{#p}.{#s}.{#w}\n",
    "SOLAR_ID = \"{http://www.w3.org/XML/1998/namespace}id\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_solar():\n",
    "    \"\"\"\n",
    "    Reads the solar corpus and returns a list of dictionaries.\n",
    "\n",
    "    Solar:\n",
    "    -> teiHeader -> /\n",
    "    -> text -> group -> text[2 = source + target] -> body ->\n",
    "        -> div[5485 = # of written works] -> bibl + p[...] ->\n",
    "        -> (date + note + term[...]) + (s[...]) -> ... + w[...] + pc[...] + seg[...]\n",
    "    -> standOff -> linkGrp[126159] -> link[...]\n",
    "\n",
    "    div = school written work\n",
    "    p = paragraph\n",
    "    s = sentence\n",
    "    w = word\n",
    "    pc = punctuation\n",
    "    seg = segment\n",
    "    \n",
    "    @return: corpus data in xml format\n",
    "    \"\"\"\n",
    "    # Read the solar data and return it\n",
    "    data_xml = read_xml(SOLAR_FILE)\n",
    "    solar_data_logger.info(\"Solar data read\")\n",
    "\n",
    "    return data_xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_solar_id(solar_id, solar_type=SolarId.D):\n",
    "    \"\"\"\n",
    "    Generates the sentence solar id, which is the same for source and target.\n",
    "\n",
    "    Solar id: solar{#div}{s-t}.{#p}.{#s}.{#w}\n",
    "\n",
    "    @param solar_id: solar id - unique id for word, sentence, paragraph and school written work\n",
    "    @param solar_type: solar word type\n",
    "    @return: generated solar id\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if solar_type == SolarId.D:\n",
    "            # Document (<div>)\n",
    "            document_id = \"\".join([number for number in solar_id if number.isdigit()])\n",
    "            return document_id\n",
    "        elif solar_type == SolarId.P:\n",
    "            # Paragraph (<p>)\n",
    "            solar_id = solar_id.split(\".\")\n",
    "            document_id = \"\".join(\n",
    "                [number for number in solar_id[0] if number.isdigit()]\n",
    "            )\n",
    "            paragraph_id = \".\".join([document_id, solar_id[1]])\n",
    "            return paragraph_id\n",
    "        elif solar_type == SolarId.S:\n",
    "            # Sentence (<s>)\n",
    "            solar_id = solar_id.split(\".\")\n",
    "            document_id = \"\".join(\n",
    "                [number for number in solar_id[0] if number.isdigit()]\n",
    "            )\n",
    "            paragraph_id = solar_id[1]\n",
    "            sentence_id = \".\".join([document_id, paragraph_id, solar_id[2]])\n",
    "            return sentence_id\n",
    "        elif solar_type == SolarId.W:\n",
    "            # Word (<w>)\n",
    "            solar_id = solar_id.split(\".\")\n",
    "            document_id = \"\".join(\n",
    "                [number for number in solar_id[0] if number.isdigit()]\n",
    "            )\n",
    "            paragraph_id = solar_id[1]\n",
    "            sentence_id = solar_id[2]\n",
    "            word_id = \".\".join([document_id, paragraph_id, sentence_id, solar_id[3]])\n",
    "            return word_id\n",
    "    except IndexError:\n",
    "        # Check if solar type is valid\n",
    "        solar_data_logger.warning(\"Invalid solar id type\")\n",
    "        return solar_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solar_data_multiple_error():\n",
    "    \"\"\"\n",
    "    Gets the solar source data, combines words into sentences with errors and\n",
    "    returns the data frame of sentence ids, sentences and error types in\n",
    "    sentences. Generated sentences contain multiple errors.\n",
    "\n",
    "    @return: a data frame of solar sentences and corresponding error types\n",
    "    \"\"\"\n",
    "    # Read the source and target token data\n",
    "    data_source = pd.read_csv(\n",
    "        SOLAR_DIRECTORY + \"solar_source_token.csv\", keep_default_na=False\n",
    "    )\n",
    "    data_target = pd.read_csv(\n",
    "        SOLAR_DIRECTORY + \"solar_target_token.csv\", keep_default_na=False\n",
    "    )\n",
    "\n",
    "    # Read the links between source and target tokens and remove entries with ID\n",
    "    data_link = pd.read_csv(SOLAR_DIRECTORY + \"solar_link.csv\")\n",
    "    data_link = data_link[data_link[\"type\"] != \"ID\"]\n",
    "\n",
    "    # Store multiple error sentence data\n",
    "    data = []\n",
    "    sentence = \"\"\n",
    "    sentence_id = \"\"\n",
    "    sentence_error = \"\"\n",
    "\n",
    "    for temp_data in data_source.itertuples():\n",
    "        # temp_data: [Index, id, text, lemma, msd_sl, msd_en, space]\n",
    "        if not sentence_id == generate_solar_id(temp_data[1], SolarId.S):\n",
    "            # Skip the first sentence errors as it is empty\n",
    "            if not sentence_id == \"\":\n",
    "                # Generate sentence id\n",
    "                temp_sentence_id = sentence_id.split(\".\")\n",
    "                temp_sentence_id_source = \"solar{}s.{}.{}\".format(*temp_sentence_id)\n",
    "                temp_sentence_id_target = \"solar{}t.{}.{}\".format(*temp_sentence_id)\n",
    "\n",
    "                # Get error type\n",
    "                temp_sentence_error_source = data_link.loc[\n",
    "                    data_link[\"source\"].str.contains(temp_sentence_id_source, na=False)\n",
    "                ]\n",
    "                temp_sentence_error_target = data_link.loc[\n",
    "                    data_link[\"target\"].str.contains(temp_sentence_id_target, na=False)\n",
    "                ]\n",
    "\n",
    "                # Join source and target error types\n",
    "                temp_sentence_error = pd.concat(\n",
    "                    [temp_sentence_error_source, temp_sentence_error_target]\n",
    "                )\n",
    "\n",
    "                # Remove duplicate error types\n",
    "                temp_sentence_error = temp_sentence_error.drop_duplicates()\n",
    "\n",
    "                # Word id might be type ID or does not exist\n",
    "                if len(temp_sentence_error):\n",
    "                    sentence_error = \";\".join(temp_sentence_error[\"type\"].tolist())\n",
    "\n",
    "            # Add sentence triplet to data and reset temp data\n",
    "            data.append([sentence_id, sentence, sentence_error])\n",
    "\n",
    "            # If sentence has error, append same sentence without any errors\n",
    "            if not sentence_error == \"\":\n",
    "                # Generate sentence id\n",
    "                temp_sentence_id = sentence_id.split(\".\")\n",
    "                temp_sentence_id_target = \"solar{}t.{}.{}\".format(*temp_sentence_id)\n",
    "\n",
    "                # Create a sentence without any errors\n",
    "                temp_sentence = \"\"\n",
    "                for temp_temp_data in data_target[\n",
    "                    data_target[\"id\"].str.contains(temp_sentence_id_target)\n",
    "                ].itertuples():\n",
    "                    temp_sentence += temp_temp_data[2] + (\n",
    "                        \" \" if temp_temp_data[6] else \"\"\n",
    "                    )\n",
    "\n",
    "                # Add sentence triplet to data and reset temp data\n",
    "                data.append([sentence_id, temp_sentence, \"\"])\n",
    "\n",
    "            sentence = \"\"\n",
    "            sentence_id = generate_solar_id(temp_data[1], SolarId.S)\n",
    "            sentence_error = \"\"\n",
    "            solar_data_logger.info(\"Processing sentence: \" + sentence_id)\n",
    "\n",
    "        sentence += temp_data[2] + (\" \" if temp_data[6] else \"\")\n",
    "\n",
    "    # Append last sentence and remove first one (empty)\n",
    "    data.append([sentence_id, sentence, sentence_error])\n",
    "    # If sentence has error, append same sentence without any errors\n",
    "    if not sentence_error == \"\":\n",
    "        # Generate sentence id\n",
    "        temp_sentence_id = sentence_id.split(\".\")\n",
    "        temp_sentence_id_target = \"solar{}t.{}.{}\".format(*temp_sentence_id)\n",
    "\n",
    "        # Create a sentence without any errors\n",
    "        temp_sentence = \"\"\n",
    "        for temp_temp_data in data_target[\n",
    "            data_target[\"id\"].str.contains(temp_sentence_id_target)\n",
    "        ].itertuples():\n",
    "            temp_sentence += temp_temp_data[2] + (\" \" if temp_temp_data[6] else \"\")\n",
    "\n",
    "        # Add sentence triplet to data and reset temp data\n",
    "        data.append([sentence_id, temp_sentence, \"\"])\n",
    "\n",
    "    data = data[1:]\n",
    "\n",
    "    df_columns = [\"id\", \"sentence\", \"error\"]\n",
    "    df_rows = data\n",
    "\n",
    "    df = pd.DataFrame(df_rows, columns=df_columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_solar_data_multiple_error():\n",
    "    \"\"\"\n",
    "    Saves the solar corpus sentences and corresponding errors to a csv file.\n",
    "    \"\"\"\n",
    "    data_multiple_error = get_solar_data_multiple_error()\n",
    "\n",
    "    data_multiple_error.to_csv(\n",
    "        SOLAR_DIRECTORY + \"solar_data_multiple_error_raw.csv\", index=False\n",
    "    )\n",
    "    solar_data_logger.info(\"Solar data with multiple errors saved to csv file\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solar_data_no_error(index, data_target, sentence, sentence_id, sentence_error):\n",
    "    \"\"\"\n",
    "    Gets the solar target data, combines words into sentences without errors and\n",
    "    returns the data frame of sentence ids, sentences and error types in sentence.\n",
    "\n",
    "    @param index: index of the sentence in the data frame\n",
    "    @param data_target: data frame of solar target data\n",
    "    @param sentence: sentence text\n",
    "    @param sentence_id: sentence id\n",
    "    @param sentence_error: sentence error type\n",
    "    @return: a triplet of solar sentences and corresponding error types\n",
    "    \"\"\"\n",
    "    while index < len(data_target):\n",
    "        temp_data = data_target.iloc[index]\n",
    "\n",
    "        # Check if sentence is over\n",
    "        if not sentence_id == generate_solar_id(temp_data[\"id\"], SolarId.S):\n",
    "            # Return a triplet of sentence id, sentence and error type\n",
    "            return [sentence_id, sentence, sentence_error]\n",
    "\n",
    "        sentence += temp_data[\"text\"] + (\" \" if temp_data[\"space\"] else \"\")\n",
    "        index += 1\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_solar_data_single_error():\n",
    "    \"\"\"\n",
    "    Gets the solar source and target data, combines words into sentences with\n",
    "    errors and returns the data frame of sentence ids, sentences and error types\n",
    "    in sentences. Generated sentences contain only one error per sentence.\n",
    "\n",
    "    We deal with three types of errors: replacement errors, deletion errors\n",
    "    and addition errors.\n",
    "\n",
    "    @return: a data frame of solar sentences and corresponding error types\n",
    "    \"\"\"\n",
    "    # Read the source and target token data\n",
    "    data_source = pd.read_csv(\n",
    "        SOLAR_DIRECTORY + \"solar_source_token.csv\", keep_default_na=False\n",
    "    )\n",
    "    data_target = pd.read_csv(\n",
    "        SOLAR_DIRECTORY + \"solar_target_token.csv\", keep_default_na=False\n",
    "    )\n",
    "\n",
    "    # Read the links between source and target tokens and remove entries with ID\n",
    "    data_link = pd.read_csv(SOLAR_DIRECTORY + \"solar_link.csv\", keep_default_na=False)\n",
    "    data_link = data_link[data_link[\"type\"] != \"ID\"]\n",
    "\n",
    "    # Change solar links so that every id wil end with ;\n",
    "    data_link[\"source\"] = data_link[\"source\"] + \";\"\n",
    "    data_link[\"target\"] = data_link[\"target\"] + \";\"\n",
    "\n",
    "    # Store single error sentence data\n",
    "    data = []\n",
    "    sentence = \"\"\n",
    "    sentence_id = \"\"\n",
    "    sentence_error = \"\"\n",
    "\n",
    "    index = 0\n",
    "    while index < len(data_target):\n",
    "        # temp_data: [Index, id, text, lemma, msd_sl, msd_en, space]\n",
    "        temp_data = data_target.iloc[index]\n",
    "\n",
    "        if not sentence_id == generate_solar_id(temp_data[\"id\"], SolarId.S):\n",
    "            # Add a triplet of sentence id and sentence without errors\n",
    "            data.append([sentence_id, sentence, sentence_error])\n",
    "            sentence = \"\"\n",
    "            sentence_id = generate_solar_id(temp_data[\"id\"], SolarId.S)\n",
    "            sentence_error = \"\"\n",
    "            solar_data_logger.info(\"Processing sentence: \" + sentence_id)\n",
    "\n",
    "        # Get error type (replacement, deletion)\n",
    "        temp_error = data_link.loc[\n",
    "            data_link[\"target\"].str.contains(temp_data[\"id\"] + \";\", na=False)\n",
    "        ]\n",
    "\n",
    "        # Check if there is an error (replacement, deletion)\n",
    "        if len(temp_error):\n",
    "            temp_error = temp_error.iloc[0]\n",
    "            # If there is an error, create a new sentence with and without error\n",
    "            if temp_error[\"source\"] == \";\" and len(temp_error[\"target\"]):\n",
    "                # Handle deletion errors\n",
    "                solar_data_logger.info(\"Dealing with: deletion error type\")\n",
    "\n",
    "                # Skip corrupting words with source tokens\n",
    "                temp_sentence = sentence\n",
    "\n",
    "                # Find the index after error\n",
    "                for temp_target_id in temp_error[\"target\"][:-1].split(\";\"):\n",
    "                    temp_target_word = data_target.loc[\n",
    "                        data_target[\"id\"] == temp_target_id\n",
    "                    ]\n",
    "                    if len(temp_target_word):\n",
    "                        temp_target_word = temp_target_word.iloc[0]\n",
    "                        index += 1\n",
    "                        sentence += temp_target_word[\"text\"] + (\n",
    "                            \" \" if temp_target_word[\"space\"] else \"\"\n",
    "                        )\n",
    "\n",
    "                # End sentence with only target tokens\n",
    "                data.append(\n",
    "                    get_solar_data_no_error(\n",
    "                        index,\n",
    "                        data_target,\n",
    "                        temp_sentence,\n",
    "                        sentence_id,\n",
    "                        temp_error[\"type\"],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            elif len(temp_error[\"source\"]) and len(temp_error[\"target\"]):\n",
    "                # Handle replacement errors\n",
    "                solar_data_logger.info(\"Dealing with: replacement error type\")\n",
    "\n",
    "                # Add source tokens to the sentence\n",
    "                temp_sentence = sentence\n",
    "                for temp_source_id in temp_error[\"source\"][:-1].split(\";\"):\n",
    "                    temp_source_word = data_source.loc[\n",
    "                        data_source[\"id\"] == temp_source_id\n",
    "                    ]\n",
    "                    if len(temp_source_word):\n",
    "                        temp_source_word = temp_source_word.iloc[0]\n",
    "                        temp_sentence += temp_source_word[\"text\"] + (\n",
    "                            \" \" if temp_source_word[\"space\"] else \"\"\n",
    "                        )\n",
    "\n",
    "                # Find the index after error\n",
    "                for temp_target_id in temp_error[\"target\"][:-1].split(\";\"):\n",
    "                    temp_target_word = data_target.loc[\n",
    "                        data_target[\"id\"] == temp_target_id\n",
    "                    ]\n",
    "                    if len(temp_target_word):\n",
    "                        temp_target_word = temp_target_word.iloc[0]\n",
    "                        index += 1\n",
    "                        sentence += temp_target_word[\"text\"] + (\n",
    "                            \" \" if temp_target_word[\"space\"] else \"\"\n",
    "                        )\n",
    "\n",
    "                # End sentence with only target tokens\n",
    "                data.append(\n",
    "                    get_solar_data_no_error(\n",
    "                        index,\n",
    "                        data_target,\n",
    "                        temp_sentence,\n",
    "                        sentence_id,\n",
    "                        temp_error[\"type\"],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            # Get error type (addition)\n",
    "            temp_error = data_link.loc[\n",
    "                data_link[\"source\"].str.contains(\n",
    "                    temp_data[\"id\"].replace(\"t.\", \"s.\") + \";\", na=False\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # Check if there is an error (addition)\n",
    "            if len(temp_error):\n",
    "                temp_error = temp_error.iloc[0]\n",
    "                # Handle addition error type\n",
    "                if temp_error[\"target\"] == \";\" and len(temp_error[\"source\"]):\n",
    "                    solar_data_logger.info(\"Dealing with: addition error type\")\n",
    "\n",
    "                    # Add source tokens to the sentence\n",
    "                    temp_sentence = sentence\n",
    "                    for temp_source_id in temp_error[\"source\"][:-1].split(\";\"):\n",
    "                        temp_source_word = data_source.loc[\n",
    "                            data_source[\"id\"] == temp_source_id\n",
    "                        ]\n",
    "                        if len(temp_source_word):\n",
    "                            temp_source_word = temp_source_word.iloc[0]\n",
    "                            temp_sentence += temp_source_word[\"text\"] + (\n",
    "                                \" \" if temp_source_word[\"space\"] else \"\"\n",
    "                            )\n",
    "\n",
    "                    # Skip correct words with target tokens\n",
    "\n",
    "                    # End sentence with only target tokens\n",
    "                    data.append(\n",
    "                        get_solar_data_no_error(\n",
    "                            index,\n",
    "                            data_target,\n",
    "                            temp_sentence,\n",
    "                            sentence_id,\n",
    "                            temp_error[\"type\"],\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                # Handle sentence without error (append target token to sentence)\n",
    "                sentence += temp_data[\"text\"] + (\" \" if temp_data[\"space\"] else \"\")\n",
    "                index += 1\n",
    "\n",
    "            else:\n",
    "                # Handle sentence without error (source or target token does not represent error)\n",
    "                sentence += temp_data[\"text\"] + (\" \" if temp_data[\"space\"] else \"\")\n",
    "                index += 1\n",
    "\n",
    "    # Append last sentence and remove first one (empty)\n",
    "    data.append([sentence_id, sentence, sentence_error])\n",
    "    data = data[1:]\n",
    "\n",
    "    df_columns = [\"id\", \"sentence\", \"error\"]\n",
    "    df_rows = data\n",
    "\n",
    "    df = pd.DataFrame(df_rows, columns=df_columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_solar_data_single_error():\n",
    "    \"\"\"\n",
    "    Saves the solar corpus sentences and corresponding errors to a csv file.\n",
    "    \"\"\"\n",
    "    data_single_error = get_solar_data_single_error()\n",
    "\n",
    "    data_single_error.to_csv(\n",
    "        SOLAR_DIRECTORY + \"solar_data_single_error_raw.csv\", index=False\n",
    "    )\n",
    "    solar_data_logger.info(\"Solar data with single errors saved to csv file\")\n",
    "\n",
    "    return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('slovko')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "0b7441cdf19e0d28b58859c3a7d82aad7fb7437a4f6af83da442de25c6af8e16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
