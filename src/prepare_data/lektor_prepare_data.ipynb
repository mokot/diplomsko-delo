{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Lektor corpus data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.reading import read_xml\n",
    "from utils.logging import get_logger\n",
    "from utils.lektor_enum import Lektor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logger\n",
    "lektor_data_logger = get_logger(\"Prepare Lektor Corpus Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "LEKTOR_DIRECTORY = \"../../data/lektor/\"\n",
    "LEKTOR_FILE = \"../../data/lektor/lektor_complete.xml\"  # complete lektor data\n",
    "MSD_INDEX_FILE = \"../../slovene_specification/MSD_index.npy\"  # MSD index\n",
    "\n",
    "# Lektor Text Encoding Initiative (TEI) TAG\n",
    "LEKTOR_TAG_ROOT = \"root\"\n",
    "LEKTOR_TAG_HEAD = \"head\"\n",
    "LEKTOR_TAG_TEXT = \"text\"\n",
    "LEKTOR_TAG_ID = \"id\"\n",
    "LEKTOR_TAG_IZVOR = \"izvor\"\n",
    "LEKTOR_TAG_SPOL_AVTORJA = \"spol-avtorja\"\n",
    "LEKTOR_TAG_IZOBRAZBA_AVTORJA = \"izobrazba-avtorja\"\n",
    "LEKTOR_TAG_STOPNJA_IZOBRAZBE = \"stopnja-izobrazbe\"\n",
    "LEKTOR_TAG_IZBORNI_JEZIK = \"izvorni-jezik\"\n",
    "LEKTOR_TAG_SPOL_LEKTORJA = \"spol-lektorja\"\n",
    "LEKTOR_TAG_STAROST_LEKTORJA = \"starost-lektorja\"\n",
    "LEKTOR_TAG_IZOBRAZBA_LEKTORJA = \"izobrazba-lektorja\"\n",
    "LEKTOR_TAG_P = \"p\"\n",
    "LEKTOR_TAG_S = \"S\"\n",
    "LEKTOR_TAG_S0 = \"s0\"  # end of sentence\n",
    "LEKTOR_TAG_S1 = \"s1\"  # beginning of sentence\n",
    "LEKTOR_TAG_W = \"w\"\n",
    "LEKTOR_TAG_C = \"c\"\n",
    "LEKTOR_TAG_LEKT = \"lekt\"\n",
    "LEKTOR_TAG_DEL = \"del\"\n",
    "LEKTOR_TAG_INS = \"ins\"\n",
    "\n",
    "# Lektor attribute name for identifying the lektor text\n",
    "# Lektor id: lektor{#div}{s-t}.{#p}.{#s}.{#w}\n",
    "LEKTOR_ID = \"LEKTOR_ID\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_lektor():\n",
    "    \"\"\"\n",
    "    Reads the lektor corpus and returns a list of dictionaries.\n",
    "\n",
    "    Lektor:\n",
    "    -> head[30 = # of written works] -> id + izvor + spol_avtorja +\n",
    "        + izobrazba-avtorja + stopnja-izobrazbe + izvorni-jezik + spol-lektorja +\n",
    "        + starost-lektorja + izobrazba-lektorja\n",
    "    -> text[30 = # of written works] -> p[...] -> w[...] + c[...]\n",
    "\n",
    "    text = written work\n",
    "    p = paragraph\n",
    "    s = space\n",
    "    w = word\n",
    "    c = punctuation\n",
    "    lekt = correction (del + ins)\n",
    "\n",
    "    @return: list of dictionaries\n",
    "    \"\"\"\n",
    "    # Read the lektor data and return it\n",
    "    data_xml = read_xml(LEKTOR_FILE)\n",
    "    lektor_data_logger.info(\"Lektor data read\")\n",
    "\n",
    "    return data_xml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lektor_lekt_data(data_xml, text=Lektor.SOURCE):\n",
    "    \"\"\"\n",
    "    Gets the lektor source data for lekt section and returns sentence and the\n",
    "    error type.\n",
    "\n",
    "    @param data_xml: lektor data in xml format\n",
    "    @param text: lektor text (source or target)\n",
    "    @return: lektor data for lekt section\n",
    "    \"\"\"\n",
    "    index_text = 0\n",
    "    if text == Lektor.TARGET:\n",
    "        index_text = 1\n",
    "    elif not text == Lektor.SOURCE:\n",
    "        lektor_data_logger.warning(\"Invalid text parameter\")\n",
    "\n",
    "    sentence = \"\"\n",
    "    sentence_error = (\n",
    "        data_xml.get(\"tip\") if not index_text else \"\"\n",
    "    )  # error type for sentence\n",
    "\n",
    "    # Loop through the list of w, c, lekt\n",
    "    for index in range(len(data_xml[index_text])):\n",
    "        if (\n",
    "            data_xml[index_text][index].tag == LEKTOR_TAG_W\n",
    "            or data_xml[index_text][index].tag == LEKTOR_TAG_C\n",
    "        ):\n",
    "            # Special case for words and punctuation\n",
    "            sentence += (\n",
    "                data_xml[index_text][index].text\n",
    "                if data_xml[index_text][index].text\n",
    "                else \"\"\n",
    "            )\n",
    "\n",
    "        elif data_xml[index_text][index].tag == LEKTOR_TAG_S:\n",
    "            # Special case for spaces\n",
    "            sentence += \" \"\n",
    "\n",
    "        elif LEKTOR_TAG_LEKT in data_xml[index_text][index].tag:\n",
    "            # Special case for lektor\n",
    "            # Recursively call this function to get the word text (nested lekt)\n",
    "            temp_sentence, temp_sentence_error = get_lektor_lekt_data(\n",
    "                data_xml[index_text][index], text\n",
    "            )\n",
    "            sentence += temp_sentence\n",
    "            sentence_error = (\n",
    "                temp_sentence_error\n",
    "                + (\";\" if len(temp_sentence_error) else \"\")\n",
    "                + sentence_error\n",
    "            )\n",
    "\n",
    "    return sentence, sentence_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lektor_data_multiple_error():\n",
    "    \"\"\"\n",
    "    Gets the lektor source and target data, combines words into sentences with\n",
    "    errors and returns the data frame of sentence ids, sentences and error\n",
    "    types in sentences. Generated sentences contain multiple errors.\n",
    "\n",
    "    @return: data frame of sentence ids, sentences and error types in sentences\n",
    "    \"\"\"\n",
    "    # Read the lektor corpus data\n",
    "    data_xml = read_lektor()\n",
    "\n",
    "    # Store multiple error sentence data\n",
    "    data = []\n",
    "    sentence = \"\"\n",
    "    sentence_id = \"\"\n",
    "    sentence_error = \"\"\n",
    "    temp_sentence = \"\"\n",
    "    sentence_counter = 0\n",
    "\n",
    "    # Loop through the list of text (skip head)\n",
    "    for index_i in range(1, len(data_xml), 2):\n",
    "        # Loop through the list of p\n",
    "        for index_j in range(len(data_xml[index_i])):\n",
    "            sentence_counter = 0  # reset the sentence counter\n",
    "            # Loop through the list of w, c, lekt\n",
    "            for index_k in range(len(data_xml[index_i][index_j])):\n",
    "                # Beginning of sentence\n",
    "                if data_xml[index_i][index_j][index_k].tag == LEKTOR_TAG_S1:\n",
    "                    # Add sentence triplet to data and reset temp data\n",
    "                    data.append([sentence_id, sentence, sentence_error])\n",
    "\n",
    "                    # If sentence contains any errors, append correct sentence\n",
    "                    if not sentence_error == \"\":\n",
    "                        data.append([sentence_id, temp_sentence, \"\"])\n",
    "\n",
    "                    sentence_counter += 1  # increase the sentence counter\n",
    "                    sentence = \"\"\n",
    "                    temp_sentence = \"\"\n",
    "                    sentence_id = \"{}.{}.{}\".format(\n",
    "                        index_i // 2 + 1, index_j + 1, sentence_counter\n",
    "                    )\n",
    "                    sentence_error = \"\"\n",
    "                    lektor_data_logger.info(\"Processing sentence: \" + sentence_id)\n",
    "\n",
    "                elif (\n",
    "                    data_xml[index_i][index_j][index_k].tag == LEKTOR_TAG_W\n",
    "                    or data_xml[index_i][index_j][index_k].tag == LEKTOR_TAG_C\n",
    "                ):\n",
    "                    # Special case for words and punctuation\n",
    "                    temp_temp_sentence = (\n",
    "                        data_xml[index_i][index_j][index_k].text\n",
    "                        if data_xml[index_i][index_j][index_k].text\n",
    "                        else \"\"\n",
    "                    )\n",
    "                    sentence += temp_temp_sentence\n",
    "                    temp_sentence += temp_temp_sentence\n",
    "\n",
    "                elif data_xml[index_i][index_j][index_k].tag == LEKTOR_TAG_S:\n",
    "                    # Special case for spaces\n",
    "                    sentence += \" \"\n",
    "                    temp_sentence += \" \"\n",
    "\n",
    "                elif LEKTOR_TAG_LEKT in data_xml[index_i][index_j][index_k].tag:\n",
    "                    # Special case for lektor\n",
    "                    temp_temp_sentence, temp_temp_sentence_error = get_lektor_lekt_data(\n",
    "                        data_xml[index_i][index_j][index_k], Lektor.SOURCE\n",
    "                    )\n",
    "                    sentence += temp_temp_sentence\n",
    "                    sentence_error += temp_temp_sentence_error + (\n",
    "                        \";\" if len(temp_temp_sentence_error) else \"\"\n",
    "                    )\n",
    "                    temp_sentence += get_lektor_lekt_data(\n",
    "                        data_xml[index_i][index_j][index_k], Lektor.TARGET\n",
    "                    )[0]\n",
    "\n",
    "    # Append last sentence and remove first one (empty)\n",
    "    data.append([sentence_id, sentence, sentence_error])\n",
    "    # If sentence contains any errors, append correct sentence\n",
    "    if not sentence_error == \"\":\n",
    "        data.append([sentence_id, temp_sentence, \"\"])\n",
    "\n",
    "    data = data[1:]\n",
    "\n",
    "    df_columns = [\"id\", \"sentence\", \"error\"]\n",
    "    df_rows = data\n",
    "\n",
    "    df = pd.DataFrame(df_rows, columns=df_columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lektor_data_multiple_error():\n",
    "    \"\"\"\n",
    "    Saves the lektor corpus sentences and corresponding errors to a csv file.\n",
    "    \"\"\"\n",
    "    data_multiple_error = get_lektor_data_multiple_error()\n",
    "\n",
    "    data_multiple_error.to_csv(\n",
    "        LEKTOR_DIRECTORY + \"lektor_data_multiple_error_raw.csv\", index=False\n",
    "    )\n",
    "    lektor_data_logger.info(\"Lektor data with multiple errors saved to csv file\")\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lektor_data_no_error(data_xml, text=Lektor.SOURCE):\n",
    "    \"\"\"\n",
    "    Gets the lektor source and target data for lekt section and returns a\n",
    "    sentences with only one error in the data frame of sentence id, sentence\n",
    "    and error type in sentence.\n",
    "\n",
    "    @param data_xml: lektor data xml\n",
    "    @param text: corpus type (source or target)\n",
    "    @return: a tuple of sentence and error type in sentence\n",
    "    \"\"\"\n",
    "    index_text = 0\n",
    "    if text == Lektor.TARGET:\n",
    "        index_text = 1\n",
    "    elif not text == Lektor.SOURCE:\n",
    "        lektor_data_logger.warning(\"Invalid text parameter\")\n",
    "\n",
    "    sentence = [\"\"]\n",
    "    sentence_error = [\n",
    "        (data_xml.get(\"tip\") if not index_text else \"\")\n",
    "    ]  # error type for sentence\n",
    "\n",
    "    # Loop through the list of w, c, lekt\n",
    "    for index in range(len(data_xml[index_text])):\n",
    "        if (\n",
    "            data_xml[index_text][index].tag == LEKTOR_TAG_W\n",
    "            or data_xml[index_text][index].tag == LEKTOR_TAG_C\n",
    "        ):\n",
    "            # Special case for words and punctuation\n",
    "            for temp_index in range(len(sentence)):\n",
    "                sentence[temp_index] += (\n",
    "                    data_xml[index_text][index].text\n",
    "                    if data_xml[index_text][index].text\n",
    "                    else \"\"\n",
    "                )\n",
    "\n",
    "        elif data_xml[index_text][index].tag == LEKTOR_TAG_S:\n",
    "            # Special case for spaces\n",
    "            for temp_index in range(len(sentence)):\n",
    "                sentence[temp_index] += \" \"\n",
    "\n",
    "        elif LEKTOR_TAG_LEKT in data_xml[index_text][index].tag:\n",
    "            # Special case for lektor\n",
    "            temp_sentence = sentence[0]\n",
    "            temp_error_sentence = sentence_error[0]\n",
    "\n",
    "            # Recursively call this function to get the word text (nested lekt)\n",
    "            # End sentence with only target tokens\n",
    "            temp_sentence_target, temp_error_target = get_lektor_data_no_error(\n",
    "                data_xml[index_text][index], Lektor.TARGET\n",
    "            )\n",
    "            for temp_index in range(len(sentence)):\n",
    "                sentence[temp_index] += temp_sentence_target[0]\n",
    "\n",
    "            # Handle error type for lekt\n",
    "            temp_sentence_source, temp_error_source = get_lektor_data_no_error(\n",
    "                data_xml[index_text][index], Lektor.SOURCE\n",
    "            )\n",
    "\n",
    "            # Add sentences before target sentence\n",
    "            for temp_index in range(len(temp_sentence_source)):\n",
    "                sentence.append(temp_sentence)\n",
    "                sentence_error.append(temp_error_sentence)\n",
    "                sentence[-1] += temp_sentence_source[temp_index]\n",
    "                sentence_error[-1] = (\n",
    "                    temp_error_source[temp_index]\n",
    "                    + (\";\" if len(temp_error_source[temp_index]) else \"\")\n",
    "                    + sentence_error[-1]\n",
    "                )\n",
    "\n",
    "    return sentence, sentence_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lektor_data_single_error():\n",
    "    \"\"\"\n",
    "    Gets the lektor source and target data, combines words into sentences with\n",
    "    errors and returns the data frame of sentence ids, sentences and error\n",
    "    types in sentences. Generated sentences contain only one error per sentence.\n",
    "\n",
    "    @return: a data frame of sentence ids, sentences and error types in sentences\n",
    "    \"\"\"\n",
    "    # Read the lektor corpus data\n",
    "    data_xml = read_lektor()\n",
    "\n",
    "    # Store multiple error sentence data\n",
    "    data = []\n",
    "    sentence = [\"\"]\n",
    "    sentence_id = \"\"\n",
    "    sentence_error = [\"\"]\n",
    "    sentence_counter = 0\n",
    "\n",
    "    # Loop through the list of text (skip head)\n",
    "    for index_i in range(1, len(data_xml), 2):\n",
    "        # Loop through the list of p\n",
    "        for index_j in range(len(data_xml[index_i])):\n",
    "            sentence_counter = 0  # reset the sentence counter\n",
    "            # Loop through the list of w, c, lekt\n",
    "            for index_k in range(len(data_xml[index_i][index_j])):\n",
    "                # Beginning of sentence\n",
    "                if data_xml[index_i][index_j][index_k].tag == LEKTOR_TAG_S1:\n",
    "                    # Add sentence triplet to data and reset temp data\n",
    "                    for index in range(len(sentence)):\n",
    "                        data.append(\n",
    "                            [sentence_id, sentence[index], sentence_error[index]]\n",
    "                        )\n",
    "                    sentence_counter += 1  # increase the sentence counter\n",
    "                    sentence = [\"\"]\n",
    "                    sentence_id = \"{}.{}.{}\".format(\n",
    "                        index_i // 2 + 1, index_j + 1, sentence_counter\n",
    "                    )\n",
    "                    sentence_error = [\"\"]\n",
    "                    lektor_data_logger.info(\"Processing sentence: \" + sentence_id)\n",
    "\n",
    "                elif (\n",
    "                    data_xml[index_i][index_j][index_k].tag == LEKTOR_TAG_W\n",
    "                    or data_xml[index_i][index_j][index_k].tag == LEKTOR_TAG_C\n",
    "                ):\n",
    "                    # Special case for words and punctuation\n",
    "                    for index in range(len(sentence)):\n",
    "                        sentence[index] += (\n",
    "                            data_xml[index_i][index_j][index_k].text\n",
    "                            if data_xml[index_i][index_j][index_k].text\n",
    "                            else \"\"\n",
    "                        )\n",
    "\n",
    "                elif data_xml[index_i][index_j][index_k].tag == LEKTOR_TAG_S:\n",
    "                    # Special case for spaces\n",
    "                    for index in range(len(sentence)):\n",
    "                        sentence[index] += \" \"\n",
    "\n",
    "                elif LEKTOR_TAG_LEKT in data_xml[index_i][index_j][index_k].tag:\n",
    "                    # Special case for lektor\n",
    "                    temp_sentence = sentence[0]\n",
    "                    temp_error_sentence = sentence_error[0]\n",
    "\n",
    "                    # End sentence with only target tokens\n",
    "                    temp_sentence_target, temp_error_target = get_lektor_data_no_error(\n",
    "                        data_xml[index_i][index_j][index_k], Lektor.TARGET\n",
    "                    )\n",
    "                    for index in range(len(sentence)):\n",
    "                        sentence[index] += temp_sentence_target[0]\n",
    "\n",
    "                    # Handle error type for lekt\n",
    "                    temp_sentence_source, temp_error_source = get_lektor_data_no_error(\n",
    "                        data_xml[index_i][index_j][index_k], Lektor.SOURCE\n",
    "                    )\n",
    "\n",
    "                    # Add sentences before target sentence\n",
    "                    for index in reversed(range(len(temp_sentence_source))):\n",
    "                        sentence.append(temp_sentence)\n",
    "                        sentence_error.append(temp_error_sentence)\n",
    "                        sentence[-1] += temp_sentence_source[index]\n",
    "                        sentence_error[-1] += temp_error_source[index] + (\n",
    "                            \";\" if len(temp_error_source[index]) else \"\"\n",
    "                        )\n",
    "\n",
    "    # Append last sentence and remove first one (empty)\n",
    "    for index in range(len(sentence)):\n",
    "        data.append([sentence_id, sentence[index], sentence_error[index]])\n",
    "    data = data[1:]\n",
    "\n",
    "    df_columns = [\"id\", \"sentence\", \"error\"]\n",
    "    df_rows = data\n",
    "\n",
    "    df = pd.DataFrame(df_rows, columns=df_columns)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_lektor_data_single_error():\n",
    "    \"\"\"\n",
    "    Saves the lektor corpus sentences and corresponding errors to a csv file.\n",
    "    \"\"\"\n",
    "    data_single_error = get_lektor_data_single_error()\n",
    "\n",
    "    data_single_error.to_csv(\n",
    "        LEKTOR_DIRECTORY + \"lektor_data_single_error_raw.csv\", index=False\n",
    "    )\n",
    "    lektor_data_logger.info(\"Lektor data with single errors saved to csv file\")\n",
    "\n",
    "    return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('slovko')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b7441cdf19e0d28b58859c3a7d82aad7fb7437a4f6af83da442de25c6af8e16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
