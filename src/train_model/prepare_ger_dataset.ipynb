{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare grammar error recognition datasets\n",
    "\n",
    "> **_NOTE:_** We are creating dataset similar to CoNLL-2003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.corpus_enum import Corpus\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    load_from_disk,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Features,\n",
    "    ClassLabel,\n",
    "    Value,\n",
    "    Sequence,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_NAME = \"cjvt/solar3\"\n",
    "SEED = 42\n",
    "TRAIN_VALIDATION_TEST_RATIO = 0.1  # split dataset: 80% - 10% - 10%\n",
    "GER_TAGS = {\n",
    "    \"0\": 0,  # NI NAPAKE\n",
    "    # 1. NAPAKA ČRKOVANJA:\n",
    "    \"Č/VOK\": 1,\n",
    "    \"Č/KONZ\": 2,\n",
    "    \"Č/W\": 3,\n",
    "    \"Č/SKLOP\": 4,\n",
    "    \"Č/PRED\": 5,\n",
    "    \"Č/PREDL\": 5,  # this one is required due to inaccuracy in solar corpus\n",
    "    # 2. NAPAKA OBLIKE:\n",
    "    \"O/KAT\": 6,\n",
    "    \"O/PAR\": 7,\n",
    "    \"O/DOD\": 8,\n",
    "    # 3. NAPAKA BESEDIŠČA:\n",
    "    \"B/SAM\": 9,\n",
    "    \"B/GLAG\": 10,\n",
    "    \"B/ZAIM\": 11,\n",
    "    \"B/PRED\": 12,\n",
    "    \"B/VEZ\": 13,\n",
    "    \"B/PRID\": 14,\n",
    "    \"B/PRISL\": 15,\n",
    "    \"B/OST\": 16,\n",
    "    \"B/MEN\": 17,\n",
    "    \"B/DOD\": 18,\n",
    "    # 4. NAPAKA SKLADNJE:\n",
    "    \"S/BR\": 19,\n",
    "    \"S/IZPUST\": 20,\n",
    "    \"S/ODVEČ\": 21,\n",
    "    \"S/STR\": 22,\n",
    "    \"S/DOD\": 23,\n",
    "    # 5. NAPAKA ZAPISA:\n",
    "    \"Z/MV\": 24,\n",
    "    \"Z/SN\": 25,\n",
    "    \"Z/KR\": 26,\n",
    "    \"Z/ŠTEV\": 27,\n",
    "    \"Z/LOČ\": 28,\n",
    "    # 6. POVEZANA NAPAKA:\n",
    "    \"P/OBL\": 29,\n",
    "    \"P/SKLA\": 30,\n",
    "    \"P/ZAP\": 31,\n",
    "    # 7. NEOPREDELJENA NAPAKA:\n",
    "    \"N/\": 32,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_sentence_id(dataset):\n",
    "    \"\"\"\n",
    "    Reset sentence id in a dataset (set it to a list from 0 to N).\n",
    "\n",
    "    @param dataset: a dataset that needs to be updated\n",
    "    @return: return updated dataset where sentence ids are from 0 to N\n",
    "    \"\"\"\n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "            \"id\": list(range(len(dataset))),\n",
    "            \"tokens\": dataset[\"tokens\"],\n",
    "            \"ger_tags\": dataset[\"ger_tags\"],\n",
    "        },\n",
    "        features=dataset.features,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus_dataset():\n",
    "    \"\"\"\n",
    "    Create a dataset for grammar error recognition. Dataset consists of\n",
    "    sentences, labels, which indicates weather the sentence contains an error\n",
    "    and sentence ids.\n",
    "\n",
    "    @return: dataset dictionary of train, validation and test set\n",
    "    \"\"\"\n",
    "    dataset = load_dataset(\n",
    "        DATASET_NAME, \"sentence_level\"\n",
    "    )  # download the source dataset (sentence level)\n",
    "\n",
    "    # Remove all unused columns and rename the used ones\n",
    "    dataset = (\n",
    "        dataset[\"train\"]\n",
    "        .remove_columns(\n",
    "            [\n",
    "                \"id_doc\",\n",
    "                \"doc_title\",\n",
    "                \"is_manually_validated\",\n",
    "                \"src_ling_annotations\",\n",
    "                \"tgt_tokens\",\n",
    "                \"tgt_ling_annotations\",\n",
    "            ]\n",
    "        )\n",
    "        .rename_columns({\"src_tokens\": \"sentence\", \"corrections\": \"error\"})\n",
    "    )\n",
    "\n",
    "    # Create a new dataframe to store tokens (words in sentence) and ger tags (error names)\n",
    "    data = pd.DataFrame(columns=[\"id\", \"tokens\", \"ger_tags\"])\n",
    "\n",
    "    for temp_data_index in range(dataset.num_rows):\n",
    "        temp_data = dataset[temp_data_index]\n",
    "\n",
    "        # The default (non error) tag is always 0\n",
    "        temp_data_error = [0] * len(temp_data[\"sentence\"])\n",
    "\n",
    "        # Loop through errors and replace them with numbers\n",
    "        for temp_error in temp_data[\"error\"]:\n",
    "            # Loop through error tokens\n",
    "            for temp_error_index in temp_error[\"idx_src\"]:\n",
    "                temp_data_error[temp_error_index] = GER_TAGS[\n",
    "                    \"/\".join(temp_error[\"corr_types\"][0].split(\"/\")[:-1])\n",
    "                ]\n",
    "\n",
    "        data = pd.concat(\n",
    "            [\n",
    "                data,\n",
    "                pd.Series(\n",
    "                    {\n",
    "                        \"tokens\": tuple(temp_data[\"sentence\"]),\n",
    "                        \"ger_tags\": tuple(temp_data_error),\n",
    "                    }\n",
    "                )\n",
    "                .to_frame()\n",
    "                .T,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Format data from [id, tokens, ger_tags] to [tokens, ger_tags]\n",
    "    data.drop_duplicates(\n",
    "        subset=[\"tokens\", \"ger_tags\"],\n",
    "        keep=\"first\",\n",
    "        inplace=True,\n",
    "        ignore_index=False,\n",
    "    )  # remove duplicates\n",
    "    data = data.reset_index()  # reset index\n",
    "    data = data.drop(columns=[\"index\", \"id\"])  # drop index and id column\n",
    "    data = data.convert_dtypes()  # convert data types into\n",
    "\n",
    "    # Create a features for dataset\n",
    "    features = Features(\n",
    "        {\n",
    "            \"id\": Value(dtype=\"int32\"),\n",
    "            \"tokens\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"ger_tags\": Sequence(\n",
    "                feature=ClassLabel(\n",
    "                    num_classes=len(GER_TAGS),\n",
    "                    names=list(GER_TAGS.keys()),\n",
    "                    id=None,\n",
    "                ),\n",
    "                length=-1,\n",
    "                id=None,\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create a dataset and specify its format\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"id\": data.index.values,\n",
    "            \"tokens\": data[\"tokens\"].values,\n",
    "            \"ger_tags\": data[\"ger_tags\"].values,\n",
    "        },\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    # Shuffle data and split it into train - validation - test set\n",
    "    dataset = dataset.shuffle(SEED)\n",
    "    dataset_train = dataset.train_test_split(test_size=2 * TRAIN_VALIDATION_TEST_RATIO)\n",
    "    dataset_validation_test = dataset_train[\"test\"].train_test_split(test_size=0.5)\n",
    "    dataset = DatasetDict(\n",
    "        {\n",
    "            \"train\": dataset_train[\"train\"],\n",
    "            \"validation\": dataset_validation_test[\"train\"],\n",
    "            \"test\": dataset_validation_test[\"test\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ger_dataset():\n",
    "    \"\"\"\n",
    "    Prepare Solar dataset, reset sentence ids and return a dataset dictionary.\n",
    "\n",
    "    @return: dataset dictionary of train, validation and test set\n",
    "    \"\"\"\n",
    "    dataset = prepare_corpus_dataset()\n",
    "\n",
    "    # Reset sentence ids for each set\n",
    "    dataset[\"train\"] = reset_sentence_id(dataset[\"train\"])\n",
    "    dataset[\"validation\"] = reset_sentence_id(dataset[\"validation\"])\n",
    "    dataset[\"test\"] = reset_sentence_id(dataset[\"test\"])\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ger_dataset(directory_name):\n",
    "    \"\"\"\n",
    "    Prepares source, target and error data and saves it to the generated files.\n",
    "\n",
    "    @param directory_name: directory name on a disk\n",
    "    @return: nothing\n",
    "    \"\"\"\n",
    "    # Prepare the model data\n",
    "    dataset = prepare_ger_dataset()\n",
    "\n",
    "    # Save the data to the disk\n",
    "    dataset.save_to_disk(directory_name)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ger_dataset(directory_name):\n",
    "    \"\"\"\n",
    "    Loads source, target and error data from a disk.\n",
    "\n",
    "    @param directory_name: directory name on a disk\n",
    "    @return: data dictionary of train, validation and test data sets\n",
    "    \"\"\"\n",
    "    # Load data from a disk\n",
    "    dataset = load_from_disk(directory_name)\n",
    "    return dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('slovko')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b7441cdf19e0d28b58859c3a7d82aad7fb7437a4f6af83da442de25c6af8e16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
