{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare grammar error detection datasets\n",
    "\n",
    "> **_NOTE:_** We are creating dataset similar to CoLA (Corpus of Linguistic Acceptability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.corpus_enum import Corpus\n",
    "from utils.error_enum import Error\n",
    "from helper_model import (\n",
    "    SOLAR_FILE_MULTIPLE_ERROR,\n",
    "    SOLAR_FILE_SINGLE_ERROR,\n",
    "    LEKTOR_FILE_MULTIPLE_ERROR,\n",
    "    LEKTOR_FILE_SINGLE_ERROR,\n",
    ")\n",
    "from datasets import (\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Features,\n",
    "    ClassLabel,\n",
    "    Value,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED = 42\n",
    "TRAIN_VALIDATION_TEST_RATIO = 0.1  # split dataset: 80% - 10% - 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_sentence_id(dataset):\n",
    "    \"\"\"\n",
    "    Reset sentence id in a dataset (set it to a list from 0 to N).\n",
    "\n",
    "    @param dataset: a dataset that needs to be updated\n",
    "    @return: return updated dataset where sentence ids are from 0 to N\n",
    "    \"\"\"\n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "            \"id\": list(range(len(dataset))),\n",
    "            \"label\": dataset[\"label\"],\n",
    "            \"sentence\": dataset[\"sentence\"],\n",
    "        },\n",
    "        features=dataset.features,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus_dataset(corpus=Corpus.SOLAR):\n",
    "    \"\"\"\n",
    "    Create a dataset for multiple and single errors. Dataset consists of\n",
    "    sentences, labels, which indicates weather the sentence contains an error\n",
    "    and sentence ids.\n",
    "\n",
    "    @param corpus: corpus type (solar or lektor)\n",
    "    @return: dataset dictionary of train, validation and test set\n",
    "    \"\"\"\n",
    "    # Set the corpus multiple and single error file paths\n",
    "    multiple_error_file, single_error_file = (\n",
    "        (\n",
    "            LEKTOR_FILE_MULTIPLE_ERROR,\n",
    "            LEKTOR_FILE_SINGLE_ERROR,\n",
    "        )\n",
    "        if corpus == Corpus.LEKTOR\n",
    "        else (\n",
    "            SOLAR_FILE_MULTIPLE_ERROR,\n",
    "            SOLAR_FILE_SINGLE_ERROR,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Read the error data from the csv files\n",
    "    multiple_error_data = pd.read_csv(multiple_error_file, keep_default_na=False)\n",
    "    single_error_data = pd.read_csv(single_error_file, keep_default_na=False)\n",
    "\n",
    "    # Join multiple and single error data\n",
    "    raw_data = pd.concat([multiple_error_data, single_error_data], ignore_index=True)\n",
    "\n",
    "    # Filter data - remove short and incorrect sentences\n",
    "    raw_data = raw_data[raw_data.sentence.str.split().apply(len) > 3]\n",
    "    raw_data = raw_data[\n",
    "        ~(\n",
    "            raw_data.sentence.str.isalnum()\n",
    "            | raw_data.sentence.str.istitle()\n",
    "            | raw_data.sentence.str.islower()\n",
    "            | raw_data.sentence.str.isupper()\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    raw_data.sentence = raw_data.sentence.apply(lambda sentence: sentence.strip())\n",
    "\n",
    "    # Replace the error types with default error keys (0 = JE NAPAKA, 1 = NI NAPAKE)\n",
    "    raw_data[\"error\"] = (raw_data[\"error\"] == \"\").astype(int)\n",
    "\n",
    "    # Format data from [id, sentence, error] to [sentence, error]\n",
    "    raw_data.drop_duplicates(\n",
    "        subset=[\"id\", \"sentence\", \"error\"],\n",
    "        keep=\"first\",\n",
    "        inplace=True,\n",
    "        ignore_index=False,\n",
    "    )  # remove duplicates\n",
    "    raw_data = raw_data.reset_index()  # reset index\n",
    "    raw_data = raw_data.drop(columns=[\"index\", \"id\"])  # drop index and id column\n",
    "    raw_data = raw_data.convert_dtypes()  # convert data types into\n",
    "\n",
    "    # Create a features for dataset\n",
    "    features = Features(\n",
    "        {\n",
    "            \"id\": Value(dtype=\"int32\"),\n",
    "            \"label\": ClassLabel(\n",
    "                num_classes=2,\n",
    "                names=[Error.ERROR_KEY.value, Error.NO_ERROR_KEY.value],\n",
    "            ),\n",
    "            \"sentence\": Value(dtype=\"string\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create a dataset and specify its format\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"id\": raw_data.index.values,\n",
    "            \"label\": raw_data[\"error\"].values,\n",
    "            \"sentence\": raw_data[\"sentence\"].values,\n",
    "        },\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    # Shuffle data and split it into train - validation - test set\n",
    "    dataset = dataset.shuffle(SEED)\n",
    "    dataset_train = dataset.train_test_split(test_size=2 * TRAIN_VALIDATION_TEST_RATIO)\n",
    "    dataset_validation_test = dataset_train[\"test\"].train_test_split(test_size=0.5)\n",
    "    dataset = DatasetDict(\n",
    "        {\n",
    "            \"train\": dataset_train[\"train\"],\n",
    "            \"validation\": dataset_validation_test[\"train\"],\n",
    "            \"test\": dataset_validation_test[\"test\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ged_dataset():\n",
    "    \"\"\"\n",
    "    Prepare Solar and Lektor dataset, combine them, reset sentence ids and return\n",
    "    a dataset dictionary.\n",
    "\n",
    "    @return: dataset dictionary of train, validation and test set\n",
    "    \"\"\"\n",
    "    solar_dataset = prepare_corpus_dataset(Corpus.SOLAR)\n",
    "    lektor_dataset = prepare_corpus_dataset(Corpus.LEKTOR)\n",
    "\n",
    "    # Combine Solar and Lektor dataset\n",
    "    dataset = DatasetDict(\n",
    "        {\n",
    "            \"train\": concatenate_datasets(\n",
    "                [solar_dataset[\"train\"], lektor_dataset[\"train\"]]\n",
    "            ).shuffle(SEED),\n",
    "            \"validation\": concatenate_datasets(\n",
    "                [solar_dataset[\"validation\"], lektor_dataset[\"validation\"]]\n",
    "            ).shuffle(SEED),\n",
    "            \"test\": concatenate_datasets(\n",
    "                [solar_dataset[\"test\"], lektor_dataset[\"test\"]]\n",
    "            ).shuffle(SEED),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Reset sentence ids for each set\n",
    "    dataset[\"train\"] = reset_sentence_id(dataset[\"train\"])\n",
    "    dataset[\"validation\"] = reset_sentence_id(dataset[\"validation\"])\n",
    "    dataset[\"test\"] = reset_sentence_id(dataset[\"test\"])\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ged_dataset(directory_name):\n",
    "    \"\"\"\n",
    "    Prepares source, target and error data and saves it to the generated files.\n",
    "\n",
    "    @param directory_name: directory name on a disk\n",
    "    @return: nothing\n",
    "    \"\"\"\n",
    "    # Prepare the model data\n",
    "    dataset = prepare_ged_dataset()\n",
    "\n",
    "    # Save the data to the disk\n",
    "    dataset.save_to_disk(directory_name)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ged_dataset(directory_name):\n",
    "    \"\"\"\n",
    "    Loads source, target and error data from a disk.\n",
    "\n",
    "    @param directory_name: directory name on a disk\n",
    "    @return: data dictionary of train, validation and test data sets\n",
    "    \"\"\"\n",
    "    # Load data from a disk\n",
    "    dataset = load_from_disk(directory_name)\n",
    "    return dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('slovko')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b7441cdf19e0d28b58859c3a7d82aad7fb7437a4f6af83da442de25c6af8e16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
