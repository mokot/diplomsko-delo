{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare grammar error correction datasets\n",
    "\n",
    "> **_NOTE:_** We are creating dataset similar to WMT (Workshop on Machine Translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils.corpus_enum import Corpus\n",
    "from helper_model import (\n",
    "    SOLAR_FILE_MULTIPLE_ERROR,\n",
    "    SOLAR_FILE_SINGLE_ERROR,\n",
    "    LEKTOR_FILE_MULTIPLE_ERROR,\n",
    "    LEKTOR_FILE_SINGLE_ERROR,\n",
    ")\n",
    "from datasets import (\n",
    "    load_from_disk,\n",
    "    concatenate_datasets,\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Features,\n",
    "    Value,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SEED = 42\n",
    "TRAIN_VALIDATION_TEST_RATIO = 0.1  # split dataset: 80% - 10% - 10%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_sentence_id(dataset):\n",
    "    \"\"\"\n",
    "    Reset sentence id in a dataset (set it to a list from 0 to N).\n",
    "\n",
    "    @param dataset: a dataset that needs to be updated\n",
    "    @return: return updated dataset where sentence ids are from 0 to N\n",
    "    \"\"\"\n",
    "    return Dataset.from_dict(\n",
    "        {\n",
    "            \"id\": list(range(len(dataset))),\n",
    "            \"source\": dataset[\"source\"],\n",
    "            \"target\": dataset[\"target\"],\n",
    "        },\n",
    "        features=dataset.features,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus_dataset(corpus=Corpus.SOLAR):\n",
    "    \"\"\"\n",
    "    Create a dataset for multiple and single errors. Dataset consists of\n",
    "    source and target sentences. Source sentences are sentences written in error\n",
    "    language, while target sentences are sentences written in non error language.\n",
    "\n",
    "    @param corpus: corpus type (solar or lektor)\n",
    "    @return: dataset dictionary of train, validation and test set\n",
    "    \"\"\"\n",
    "    # Set the corpus multiple and single error file paths\n",
    "    multiple_error_file, single_error_file = (\n",
    "        (\n",
    "            LEKTOR_FILE_MULTIPLE_ERROR,\n",
    "            LEKTOR_FILE_SINGLE_ERROR,\n",
    "        )\n",
    "        if corpus == Corpus.LEKTOR\n",
    "        else (\n",
    "            SOLAR_FILE_MULTIPLE_ERROR,\n",
    "            SOLAR_FILE_SINGLE_ERROR,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Read the error data from the csv files\n",
    "    multiple_error_data = pd.read_csv(multiple_error_file, keep_default_na=False)\n",
    "    single_error_data = pd.read_csv(single_error_file, keep_default_na=False)\n",
    "\n",
    "    # Join multiple and single error data\n",
    "    raw_data = pd.concat([multiple_error_data, single_error_data], ignore_index=True)\n",
    "\n",
    "    # Filter data - remove short and incorrect sentences\n",
    "    raw_data = raw_data[raw_data.sentence.str.split().apply(len) > 3]\n",
    "    raw_data = raw_data[\n",
    "        ~(\n",
    "            raw_data.sentence.str.isalnum()\n",
    "            | raw_data.sentence.str.istitle()\n",
    "            | raw_data.sentence.str.islower()\n",
    "            | raw_data.sentence.str.isupper()\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Remove extra spaces\n",
    "    raw_data.sentence = raw_data.sentence.apply(lambda sentence: sentence.strip())\n",
    "\n",
    "    # Separate the error and no error data\n",
    "    data_error = raw_data[raw_data[\"error\"] != \"\"]\n",
    "    data_no_error = raw_data[raw_data[\"error\"] == \"\"]\n",
    "\n",
    "    # Create a new dataframe to store source (with errors) and target (without error) sentences\n",
    "    data = pd.DataFrame(columns=[\"id\", \"source\", \"target\"])\n",
    "\n",
    "    for temp_data in data_error.itertuples():\n",
    "        # 0 = Index, 1 = id, 2 = sentence, 3 = error\n",
    "        target = data_no_error[data_no_error[\"id\"] == temp_data[1]]\n",
    "        # Skip the case where we have only one correct sentence\n",
    "        if not len(target):\n",
    "            data = pd.concat(\n",
    "                [\n",
    "                    data,\n",
    "                    pd.Series(\n",
    "                        {\n",
    "                            \"source\": temp_data[2],\n",
    "                            \"target\": temp_data[2],\n",
    "                        }\n",
    "                    )\n",
    "                    .to_frame()\n",
    "                    .T,\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        data = pd.concat(\n",
    "            [\n",
    "                data,\n",
    "                pd.Series(\n",
    "                    {\n",
    "                        \"source\": temp_data[2],\n",
    "                        \"target\": target.iloc[0][\"sentence\"],\n",
    "                    }\n",
    "                )\n",
    "                .to_frame()\n",
    "                .T,\n",
    "            ],\n",
    "            ignore_index=True,\n",
    "        )\n",
    "\n",
    "    # Format data from [id, source, target] to [source, target]\n",
    "    data.drop_duplicates(\n",
    "        subset=[\"source\", \"target\"],\n",
    "        keep=\"first\",\n",
    "        inplace=True,\n",
    "        ignore_index=False,\n",
    "    )  # remove duplicates\n",
    "    data = data.reset_index()  # reset index\n",
    "    data = data.drop(columns=[\"id\"])  # drop index and id column\n",
    "    data = data.convert_dtypes()  # convert data types into\n",
    "\n",
    "    # Create a features for dataset\n",
    "    features = Features(\n",
    "        {\n",
    "            \"id\": Value(dtype=\"int32\"),\n",
    "            \"source\": Value(dtype=\"string\"),\n",
    "            \"target\": Value(dtype=\"string\"),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Create a dataset and specify its format\n",
    "    dataset = Dataset.from_dict(\n",
    "        {\n",
    "            \"id\": data.index.values,\n",
    "            \"source\": data[\"source\"].values,\n",
    "            \"target\": data[\"target\"].values,\n",
    "        },\n",
    "        features=features,\n",
    "    )\n",
    "\n",
    "    # Shuffle data and split it into train - validation - test set\n",
    "    dataset = dataset.shuffle(SEED)\n",
    "    dataset_train = dataset.train_test_split(test_size=2 * TRAIN_VALIDATION_TEST_RATIO)\n",
    "    dataset_validation_test = dataset_train[\"test\"].train_test_split(test_size=0.5)\n",
    "    dataset = DatasetDict(\n",
    "        {\n",
    "            \"train\": dataset_train[\"train\"],\n",
    "            \"validation\": dataset_validation_test[\"train\"],\n",
    "            \"test\": dataset_validation_test[\"test\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_gec_dataset():\n",
    "    \"\"\"\n",
    "    Prepare Solar and Lektor dataset, combine them, reset sentence ids and return\n",
    "    a dataset dictionary.\n",
    "\n",
    "    @return: dataset dictionary of train, validation and test set\n",
    "    \"\"\"\n",
    "    solar_dataset = prepare_corpus_dataset(Corpus.SOLAR)\n",
    "    lektor_dataset = prepare_corpus_dataset(Corpus.LEKTOR)\n",
    "\n",
    "    # Combine Solar and Lektor dataset\n",
    "    dataset = DatasetDict(\n",
    "        {\n",
    "            \"train\": concatenate_datasets(\n",
    "                [solar_dataset[\"train\"], lektor_dataset[\"train\"]]\n",
    "            ).shuffle(SEED),\n",
    "            \"validation\": concatenate_datasets(\n",
    "                [solar_dataset[\"validation\"], lektor_dataset[\"validation\"]]\n",
    "            ).shuffle(SEED),\n",
    "            \"test\": concatenate_datasets(\n",
    "                [solar_dataset[\"test\"], lektor_dataset[\"test\"]]\n",
    "            ).shuffle(SEED),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Reset sentence ids for each set\n",
    "    dataset[\"train\"] = reset_sentence_id(dataset[\"train\"])\n",
    "    dataset[\"validation\"] = reset_sentence_id(dataset[\"validation\"])\n",
    "    dataset[\"test\"] = reset_sentence_id(dataset[\"test\"])\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gec_dataset(directory_name):\n",
    "    \"\"\"\n",
    "    Prepares source, target and error data and saves it to the generated files.\n",
    "\n",
    "    @param directory_name: directory name on a disk\n",
    "    @return: nothing\n",
    "    \"\"\"\n",
    "    # Prepare the model data\n",
    "    dataset = prepare_gec_dataset()\n",
    "\n",
    "    # Save the data to the disk\n",
    "    dataset.save_to_disk(directory_name)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gec_dataset(directory_name):\n",
    "    \"\"\"\n",
    "    Loads source, target and error data from a disk.\n",
    "\n",
    "    @param directory_name: directory name on a disk\n",
    "    @return: data dictionary of train, validation and test data sets\n",
    "    \"\"\"\n",
    "    # Load data from a disk\n",
    "    dataset = load_from_disk(directory_name)\n",
    "    return dataset\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('slovko')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b7441cdf19e0d28b58859c3a7d82aad7fb7437a4f6af83da442de25c6af8e16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
