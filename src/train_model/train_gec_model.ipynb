{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train grammar error correction model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from utils.logging import get_logger\n",
    "from utils.metrics import (\n",
    "    metric_bleu,\n",
    "    metric_sacrebleu,\n",
    "    metric_gleu,\n",
    "    metric_chrf,\n",
    "    metric_meteor,\n",
    "    metric_ter,\n",
    "    metric_cer,\n",
    "    metric_wer,\n",
    ")\n",
    "from helper_model import GEC_MODEL, GEC_DIRECTORY\n",
    "from prepare_gec_dataset import load_gec_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get logger\n",
    "train_gec_model = get_logger(\"Train GEC model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_CHECKPOINT = \"cjvt/t5-sl-small\"\n",
    "MODEL_NAME = GEC_MODEL\n",
    "BATCH_SIZE = 16  # 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the GEC dataset\n",
    "dataset = load_gec_dataset(GEC_DIRECTORY)\n",
    "train_gec_model.info(\"{} dataset read\".format(MODEL_NAME))\n",
    "\n",
    "# Create the tokenizer and the model for our model (SloBERTa)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, use_fast=True)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "train_gec_model.info(\"{} model and tokenizer initialized\".format(MODEL_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the code device-agnostic\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transferring the model to a CUDA enabled GPU\n",
    "model = model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(data):\n",
    "    \"\"\"\n",
    "    Tokenize sentences with specific tokenizer which suits our model. Tokenizer\n",
    "    will tokenize text inputs and put it in a format the model excepts, as well\n",
    "    as generate the other inputs that model generates\n",
    "\n",
    "    NB: use text target to tokenize our labels\n",
    "    NB: we use truncation to ensure that the input longer than what the model\n",
    "    can handle will be truncated to the maximum length accepted by the model.\n",
    "    NB: we used batched processing to leverage the full benefit of the fast\n",
    "    tokenizer.\n",
    "\n",
    "    @param data: the data we want to tokenize\n",
    "    @return: tokenized data with a specific model required tokenizer\n",
    "    \"\"\"\n",
    "    data = tokenizer(data[\"source\"], text_target=data[\"target\"], truncation=True)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply tokenize function on all the sentences in our dataset\n",
    "encoded_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Setup the training arguments\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=MODEL_NAME,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"gleu\",\n",
    "    greater_is_better=True,\n",
    "    auto_find_batch_size=True,\n",
    "    report_to=\"all\",\n",
    "    predict_with_generate=True,\n",
    "    deepspeed=\"./deepspeed_config.json\",\n",
    ")\n",
    "\n",
    "# Data collator, which will pad the inputs and the labels to the max length\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Get a predictions, which need to be evaluated, and evaluate them with specific\n",
    "    metric.\n",
    "\n",
    "    @param eval_pred: the predictions, which needs to be evaluated\n",
    "    @return: evaluation score\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    print(predictions, labels)\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post processing\n",
    "    decoded_predictions = [\n",
    "        temp_prediction.strip() for temp_prediction in decoded_predictions\n",
    "    ]\n",
    "    decoded_labels = [[temp_label.strip()] for temp_label in decoded_labels]\n",
    "\n",
    "    bleu = metric_bleu.compute(\n",
    "        predictions=decoded_predictions, references=decoded_labels\n",
    "    )[\"bleu\"]\n",
    "\n",
    "    sacrebleu = metric_sacrebleu.compute(\n",
    "        predictions=decoded_predictions, references=decoded_labels\n",
    "    )[\"score\"]\n",
    "\n",
    "    gleu = metric_gleu.compute(\n",
    "        predictions=decoded_predictions, references=decoded_labels\n",
    "    )[\"google_bleu\"]\n",
    "\n",
    "    chrf = metric_chrf.compute(\n",
    "        predictions=decoded_predictions, references=decoded_labels\n",
    "    )[\"score\"]\n",
    "\n",
    "    meteor = metric_meteor.compute(\n",
    "        predictions=decoded_predictions, references=decoded_labels\n",
    "    )[\"meteor\"]\n",
    "\n",
    "    ter = metric_ter.compute(\n",
    "        predictions=decoded_predictions, references=decoded_labels\n",
    "    )[\"score\"]\n",
    "\n",
    "    cer = metric_cer.compute(\n",
    "        predictions=decoded_predictions,\n",
    "        references=[\n",
    "            decoded_label for element in decoded_labels for decoded_label in element\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    wer = metric_wer.compute(\n",
    "        predictions=decoded_predictions,\n",
    "        references=[\n",
    "            decoded_label for element in decoded_labels for decoded_label in element\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    prediction_lengths = [\n",
    "        np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu,\n",
    "        \"sacrebleu\": sacrebleu,\n",
    "        \"gleu\": gleu,\n",
    "        \"chrf\": chrf,\n",
    "        \"meteor\": meteor,\n",
    "        \"ter\": ter,\n",
    "        \"cer\": cer,\n",
    "        \"wer\": wer,\n",
    "        \"prediction_lengths\": np.mean(prediction_lengths),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    \"\"\"\n",
    "    Create a model for sequence classification with two labels.\n",
    "    @return: a model, which we will fine tune\n",
    "    \"\"\"\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter search\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "train_gec_model.info(\"{} trainer initialized\".format(MODEL_NAME))\n",
    "\n",
    "# Find most optimal parameters for our model\n",
    "train_gec_model.info(\"{} GEC hyperparameter search started\".format(MODEL_NAME))\n",
    "hyperparameters = trainer.hyperparameter_search(direction=\"maximize\")\n",
    "train_gec_model.info(\"{} GEC hyperparameter search ended\".format(MODEL_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use most optimal parameters\n",
    "for name, value in hyperparameters.hyperparameters.items():\n",
    "    setattr(trainer.args, name, value)\n",
    "train_gec_model.info(\"Hyperparameters: {}\".format(hyperparameters.hyperparameters))\n",
    "\n",
    "# Fine tune the model for GEC task\n",
    "train_gec_model.info(\"{} model training started\".format(MODEL_NAME))\n",
    "trainer.train()\n",
    "train_gec_model.info(\"{} model training ended\".format(MODEL_NAME))\n",
    "\n",
    "# Check if the trainer did reload the best model and not the last\n",
    "train_gec_model.info(trainer.evaluate())\n",
    "\n",
    "# Save the model so it can be reloaded with from_pretrained()\n",
    "trainer.save_model(MODEL_NAME)\n",
    "train_gec_model.info(\"{} model saved\".format(MODEL_NAME))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('slovko')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b7441cdf19e0d28b58859c3a7d82aad7fb7437a4f6af83da442de25c6af8e16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
